# Identifying Key Entities in Recipe Data

This repository contains a Jupyter notebook and supporting code to train a Named Entity Recognition (NER) model that extracts key entities (quantities, units, ingredients, etc.) from recipe ingredient lines. The project demonstrates data preparation, feature engineering, training, evaluation, and saving/loading a Conditional Random Fields (CRF) model using sklearn-crfsuite.

Main notebook
- files/Identifying_Key_Entities_Recipe_Data.ipynb

Quick summary
- Goal: Train a CRF-based NER model that labels tokens in recipe ingredient strings as `quantity`, `unit`, `ingredient`, and other token-level labels.
- Data: A JSON file (ingredient_and_quantity.json) containing records with `input` (raw ingredient string) and `pos` (token-level labels). The notebook tokenizes these into `input_tokens` and `pos_tokens` for training.
- Model: Uses `sklearn-crfsuite` to train a sequence-labeling model. The notebook includes feature extraction, model training, evaluation (classification report, confusion matrix), and model persistence.

How to run

Option A — Run in Google Colab (recommended)
1. Open the notebook `files/Identifying_Key_Entities_Recipe_Data.ipynb` in Colab.
2. Ensure your dataset (`ingredient_and_quantity.json`) is accessible — the notebook expects it at `/content/drive/MyDrive/ingredient_and_quantity.json` by default when mounting Google Drive. Update the path in the notebook if needed.
3. Run cells sequentially. The notebook installs `sklearn-crfsuite` and mounts Google Drive where required.

Option B — Run locally
1. Create a Python environment and install dependencies:
   pip install pandas scikit-learn sklearn-crfsuite spacy joblib matplotlib seaborn
2. Place `ingredient_and_quantity.json` in the working directory or update the notebook's data path.
3. Launch Jupyter and open `files/Identifying_Key_Entities_Recipe_Data.ipynb`.

Dependencies
- Python 3.8+
- pandas
- scikit-learn
- sklearn-crfsuite
- spaCy (optional for additional NLP utilities)
- joblib
- matplotlib, seaborn

Notebook structure (high level)
1. Imports and environment setup
2. Data ingestion and exploration
3. Tokenization: `input` -> `input_tokens`, `pos` -> `pos_tokens`
4. Feature extraction for sequence labeling
5. Train/test split and CRF model training
6. Evaluation and analysis (classification reports, confusion matrix, etc.)
7. Save/load trained model

Notes and tips
- The notebook currently demonstrates reading from Google Drive. If you prefer storing the dataset inside the repo, move `ingredient_and_quantity.json` into a `data/` folder (e.g., `data/ingredient_and_quantity.json`) and update the notebook's file path.
- Experiment with additional token-level features (capitalization, suffix/prefix, token shape, neighboring tokens, POS from spaCy) and with hyperparameters for `sklearn_crfsuite` to improve model performance.
- Consider cross-validation and class-weight handling if label imbalance affects results.

Contributing
- Open issues or pull requests for improvements, bug fixes, or feature additions.
- If adding data files to the repo, avoid committing large datasets directly; consider using a data hosting service or Git LFS.

License
- MIT License — feel free to reuse and adapt the code.

Contact
- For suggestions or questions, open an issue or contact `sp-coding-enthusiast` on GitHub.
